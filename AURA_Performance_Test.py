#!/usr/bin/env python3
"""
AURA ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ì‹¤í—˜ ëª¨ë“ˆ
ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ì„±ëŠ¥ ì§€í‘œë“¤ì„ ì¸¡ì •í•˜ê³  ê²€ì¦í•©ë‹ˆë‹¤.

ì‘ì„±ì: ìœ¤ì¢…ì„ Ã— GPT-4o ê¸°ë°˜ ê³µë™ ì„¤ê³„
ì‘ì„±ì¼: 2024ë…„ 5ì›”
"""

import asyncio
import time
import json
import logging
from datetime import datetime
from typing import Dict, List, Any, Tuple
import statistics
from dataclasses import dataclass
import matplotlib.pyplot as plt
import numpy as np

from AURA_Complete_Framework import AURACompleteFramework, process_with_aura

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ì§€í‘œ"""
    avg_processing_time: float
    avg_confidence_score: float
    recall_accuracy: float
    insight_quality: float
    wisdom_effectiveness: float
    truth_detection_rate: float
    existence_sense_level: float
    token_efficiency: float
    memory_retrieval_speed: float
    intuition_accuracy: float

class AURAPerformanceTester:
    """AURA ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.framework = None
        self.test_results = []
        self.baseline_metrics = {
            "avg_processing_time": 340,  # ms (ê¸°ì¡´ êµ¬ì¡°)
            "avg_confidence_score": 0.426,  # 42.6%
            "recall_accuracy": 0.54,  # 54%
            "token_efficiency": 1200,  # í† í° ìˆ˜
            "memory_retrieval_speed": 340  # ms
        }
        
    async def initialize(self):
        """í…ŒìŠ¤í„° ì´ˆê¸°í™”"""
        try:
            self.framework = AURACompleteFramework()
            await self.framework.initialize()
            logger.info("âœ… AURA ì„±ëŠ¥ í…ŒìŠ¤í„° ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def run_comprehensive_test(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸ§ª AURA ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘ - {len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤")
            
            results = []
            for i, test_case in enumerate(test_cases):
                logger.info(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i+1}/{len(test_cases)}: {test_case['description']}")
                
                result = await self._run_single_test(test_case)
                results.append(result)
                
                # ì§„í–‰ë¥  í‘œì‹œ
                if (i + 1) % 10 == 0:
                    logger.info(f"ğŸ“Š ì§„í–‰ë¥ : {i+1}/{len(test_cases)} ({((i+1)/len(test_cases)*100):.1f}%)")
            
            # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
            metrics = self._calculate_performance_metrics(results)
            
            # ê°œì„ ë¥  ê³„ì‚°
            improvements = self._calculate_improvements(metrics)
            
            # ê²°ê³¼ ì €ì¥
            test_summary = {
                "timestamp": datetime.now().isoformat(),
                "total_tests": len(test_cases),
                "metrics": metrics,
                "improvements": improvements,
                "detailed_results": results
            }
            
            # ê²°ê³¼ íŒŒì¼ ì €ì¥
            with open("aura_performance_test_results.json", "w", encoding="utf-8") as f:
                json.dump(test_summary, f, ensure_ascii=False, indent=2)
            
            logger.info("âœ… ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            return test_summary
            
        except Exception as e:
            logger.error(f"âŒ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            raise
    
    async def _run_single_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            start_time = time.time()
            
            # AURA ì‹œìŠ¤í…œ ì²˜ë¦¬
            result = await process_with_aura(
                user_input=test_case["input"],
                context=test_case.get("context", {})
            )
            
            end_time = time.time()
            processing_time = (end_time - start_time) * 1000  # msë¡œ ë³€í™˜
            
            # í† í° íš¨ìœ¨ì„± ê³„ì‚°
            token_count = self._estimate_token_count(result.wisdom_response)
            
            # íšŒìƒ ì •í™•ë„ í‰ê°€
            recall_accuracy = self._evaluate_recall_accuracy(
                result.recalled_memories, 
                test_case["input"],
                test_case.get("expected_tags", [])
            )
            
            # í†µì°° í’ˆì§ˆ í‰ê°€
            insight_quality = self._evaluate_insight_quality(result.insights)
            
            # ì§€í˜œ íš¨ê³¼ì„± í‰ê°€
            wisdom_effectiveness = self._evaluate_wisdom_effectiveness(
                result.wisdom_response,
                test_case.get("expected_wisdom_level", "medium")
            )
            
            # ì§„ë¦¬ íƒì§€ìœ¨ í‰ê°€
            truth_detection_rate = result.truth_detected.get("overall_confidence", 0.0)
            
            # ì¡´ì¬ ê°ê° ìˆ˜ì¤€
            existence_sense_level = result.existence_sense.get("existence_level", 0.0)
            
            # ì§ê° ì •í™•ë„ í‰ê°€
            intuition_accuracy = self._evaluate_intuition_accuracy(
                result.recalled_memories,
                test_case["input"]
            )
            
            return {
                "test_case": test_case,
                "processing_time_ms": processing_time,
                "confidence_score": result.confidence_score,
                "recall_accuracy": recall_accuracy,
                "insight_quality": insight_quality,
                "wisdom_effectiveness": wisdom_effectiveness,
                "truth_detection_rate": truth_detection_rate,
                "existence_sense_level": existence_sense_level,
                "token_count": token_count,
                "intuition_accuracy": intuition_accuracy,
                "recalled_memories_count": len(result.recalled_memories),
                "insights_count": len(result.insights)
            }
            
        except Exception as e:
            logger.error(f"âŒ ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {
                "test_case": test_case,
                "error": str(e),
                "processing_time_ms": 0,
                "confidence_score": 0,
                "recall_accuracy": 0,
                "insight_quality": 0,
                "wisdom_effectiveness": 0,
                "truth_detection_rate": 0,
                "existence_sense_level": 0,
                "token_count": 0,
                "intuition_accuracy": 0,
                "recalled_memories_count": 0,
                "insights_count": 0
            }
    
    def _calculate_performance_metrics(self, results: List[Dict[str, Any]]) -> PerformanceMetrics:
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        try:
            # ìœ íš¨í•œ ê²°ê³¼ë§Œ í•„í„°ë§
            valid_results = [r for r in results if "error" not in r]
            
            if not valid_results:
                return PerformanceMetrics(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
            
            # í‰ê·  ì²˜ë¦¬ ì‹œê°„
            avg_processing_time = statistics.mean([r["processing_time_ms"] for r in valid_results])
            
            # í‰ê·  ì‹ ë¢°ë„ ì ìˆ˜
            avg_confidence_score = statistics.mean([r["confidence_score"] for r in valid_results])
            
            # íšŒìƒ ì •í™•ë„
            recall_accuracy = statistics.mean([r["recall_accuracy"] for r in valid_results])
            
            # í†µì°° í’ˆì§ˆ
            insight_quality = statistics.mean([r["insight_quality"] for r in valid_results])
            
            # ì§€í˜œ íš¨ê³¼ì„±
            wisdom_effectiveness = statistics.mean([r["wisdom_effectiveness"] for r in valid_results])
            
            # ì§„ë¦¬ íƒì§€ìœ¨
            truth_detection_rate = statistics.mean([r["truth_detection_rate"] for r in valid_results])
            
            # ì¡´ì¬ ê°ê° ìˆ˜ì¤€
            existence_sense_level = statistics.mean([r["existence_sense_level"] for r in valid_results])
            
            # í† í° íš¨ìœ¨ì„± (í‰ê·  í† í° ìˆ˜)
            token_efficiency = statistics.mean([r["token_count"] for r in valid_results])
            
            # ê¸°ì–µ íšŒìƒ ì†ë„ (ì²˜ë¦¬ ì‹œê°„ê³¼ ë™ì¼)
            memory_retrieval_speed = avg_processing_time
            
            # ì§ê° ì •í™•ë„
            intuition_accuracy = statistics.mean([r["intuition_accuracy"] for r in valid_results])
            
            return PerformanceMetrics(
                avg_processing_time=avg_processing_time,
                avg_confidence_score=avg_confidence_score,
                recall_accuracy=recall_accuracy,
                insight_quality=insight_quality,
                wisdom_effectiveness=wisdom_effectiveness,
                truth_detection_rate=truth_detection_rate,
                existence_sense_level=existence_sense_level,
                token_efficiency=token_efficiency,
                memory_retrieval_speed=memory_retrieval_speed,
                intuition_accuracy=intuition_accuracy
            )
            
        except Exception as e:
            logger.error(f"âŒ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return PerformanceMetrics(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
    
    def _calculate_improvements(self, metrics: PerformanceMetrics) -> Dict[str, float]:
        """ê°œì„ ë¥  ê³„ì‚°"""
        try:
            improvements = {}
            
            # ì²˜ë¦¬ ì‹œê°„ ê°œì„ ë¥  (ë¹ ë¥¼ìˆ˜ë¡ ì¢‹ìŒ)
            time_improvement = ((self.baseline_metrics["avg_processing_time"] - metrics.avg_processing_time) / 
                              self.baseline_metrics["avg_processing_time"]) * 100
            improvements["processing_time_improvement"] = time_improvement
            
            # ì‹ ë¢°ë„ ê°œì„ ë¥ 
            confidence_improvement = ((metrics.avg_confidence_score - self.baseline_metrics["avg_confidence_score"]) / 
                                    self.baseline_metrics["avg_confidence_score"]) * 100
            improvements["confidence_improvement"] = confidence_improvement
            
            # íšŒìƒ ì •í™•ë„ ê°œì„ ë¥ 
            recall_improvement = ((metrics.recall_accuracy - self.baseline_metrics["recall_accuracy"]) / 
                                self.baseline_metrics["recall_accuracy"]) * 100
            improvements["recall_accuracy_improvement"] = recall_improvement
            
            # í† í° íš¨ìœ¨ì„± ê°œì„ ë¥  (ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
            token_improvement = ((self.baseline_metrics["token_efficiency"] - metrics.token_efficiency) / 
                               self.baseline_metrics["token_efficiency"]) * 100
            improvements["token_efficiency_improvement"] = token_improvement
            
            # ê¸°ì–µ íšŒìƒ ì†ë„ ê°œì„ ë¥ 
            retrieval_improvement = ((self.baseline_metrics["memory_retrieval_speed"] - metrics.memory_retrieval_speed) / 
                                   self.baseline_metrics["memory_retrieval_speed"]) * 100
            improvements["memory_retrieval_speed_improvement"] = retrieval_improvement
            
            return improvements
            
        except Exception as e:
            logger.error(f"âŒ ê°œì„ ë¥  ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def _estimate_token_count(self, text: str) -> int:
        """í† í° ìˆ˜ ì¶”ì •"""
        try:
            # ê°„ë‹¨í•œ í† í° ìˆ˜ ì¶”ì • (ì‹¤ì œë¡œëŠ” tiktoken ì‚¬ìš© ê¶Œì¥)
            words = text.split()
            return len(words) * 1.3  # í‰ê· ì ìœ¼ë¡œ ë‹¨ì–´ë‹¹ 1.3 í† í°
        except Exception as e:
            logger.error(f"âŒ í† í° ìˆ˜ ì¶”ì • ì‹¤íŒ¨: {e}")
            return 0
    
    def _evaluate_recall_accuracy(self, recalled_memories: List[Dict[str, Any]], 
                                 user_input: str, expected_tags: List[str]) -> float:
        """íšŒìƒ ì •í™•ë„ í‰ê°€"""
        try:
            if not recalled_memories:
                return 0.0
            
            # ì‚¬ìš©ì ì…ë ¥ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            input_words = set(user_input.lower().split())
            
            # ê¸°ëŒ€ íƒœê·¸ì™€ ì‹¤ì œ íšŒìƒëœ ë‚´ìš© ë¹„êµ
            tag_match_score = 0.0
            if expected_tags:
                for memory in recalled_memories:
                    memory_content = memory.get("content", "").lower()
                    memory_tags = memory.get("tags", [])
                    
                    # íƒœê·¸ ë§¤ì¹­
                    tag_matches = sum(1 for tag in expected_tags if tag.lower() in memory_content)
                    tag_match_score += tag_matches / len(expected_tags)
                
                tag_match_score /= len(recalled_memories)
            
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            keyword_match_score = 0.0
            for memory in recalled_memories:
                memory_content = memory.get("content", "").lower()
                memory_words = set(memory_content.split())
                
                # í‚¤ì›Œë“œ ê²¹ì¹¨ ê³„ì‚°
                overlap = len(input_words.intersection(memory_words))
                keyword_match_score += overlap / max(len(input_words), 1)
            
            keyword_match_score /= len(recalled_memories)
            
            # ì¢…í•© ì ìˆ˜
            total_score = (tag_match_score * 0.6 + keyword_match_score * 0.4)
            return min(total_score, 1.0)
            
        except Exception as e:
            logger.error(f"âŒ íšŒìƒ ì •í™•ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _evaluate_insight_quality(self, insights: List[str]) -> float:
        """í†µì°° í’ˆì§ˆ í‰ê°€"""
        try:
            if not insights:
                return 0.0
            
            quality_scores = []
            for insight in insights:
                score = 0.0
                
                # ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´)
                length = len(insight)
                if 20 <= length <= 200:
                    score += 0.3
                elif length > 200:
                    score += 0.1
                
                # í‚¤ì›Œë“œ ë‹¤ì–‘ì„± ì ìˆ˜
                words = insight.split()
                unique_words = len(set(words))
                diversity_score = unique_words / max(len(words), 1)
                score += diversity_score * 0.3
                
                # ì˜ë¯¸ ìˆëŠ” í‚¤ì›Œë“œ í¬í•¨ ì ìˆ˜
                meaningful_keywords = ["ì´í•´", "ê´€ê³„", "íŒ¨í„´", "ì˜ë¯¸", "ì—°ê²°", "ë°œê²¬", "í†µì°°", "ë¶„ì„"]
                keyword_count = sum(1 for word in words if word in meaningful_keywords)
                score += min(keyword_count * 0.1, 0.4)
                
                quality_scores.append(score)
            
            return statistics.mean(quality_scores)
            
        except Exception as e:
            logger.error(f"âŒ í†µì°° í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _evaluate_wisdom_effectiveness(self, wisdom_response: str, expected_level: str) -> float:
        """ì§€í˜œ íš¨ê³¼ì„± í‰ê°€"""
        try:
            if not wisdom_response:
                return 0.0
            
            score = 0.0
            
            # ì§€í˜œ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì ìˆ˜
            wisdom_keywords = ["ì§€í˜œ", "í†µì°°", "ì´í•´", "ê´€ì ", "ìƒê°", "ê³ ë ¤", "ê· í˜•", "ì¡°í™”"]
            response_lower = wisdom_response.lower()
            keyword_count = sum(1 for keyword in wisdom_keywords if keyword in response_lower)
            score += min(keyword_count * 0.2, 0.6)
            
            # ì‘ë‹µ ê¸¸ì´ ì ìˆ˜ (ì ì ˆí•œ ê¸¸ì´)
            length = len(wisdom_response)
            if 50 <= length <= 300:
                score += 0.2
            elif length > 300:
                score += 0.1
            
            # ê¸°ëŒ€ ìˆ˜ì¤€ì— ë”°ë¥¸ ì¶”ê°€ ì ìˆ˜
            if expected_level == "high" and score > 0.5:
                score += 0.2
            elif expected_level == "medium" and score > 0.3:
                score += 0.1
            
            return min(score, 1.0)
            
        except Exception as e:
            logger.error(f"âŒ ì§€í˜œ íš¨ê³¼ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _evaluate_intuition_accuracy(self, recalled_memories: List[Dict[str, Any]], user_input: str) -> float:
        """ì§ê° ì •í™•ë„ í‰ê°€"""
        try:
            if not recalled_memories:
                return 0.0
            
            # ê³µëª… ì ìˆ˜ ê¸°ë°˜ ì§ê° ì •í™•ë„
            resonance_scores = []
            for memory in recalled_memories:
                resonance = memory.get("resonance_score", 50) / 100.0
                resonance_scores.append(resonance)
            
            # í‰ê·  ê³µëª… ì ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ì§ê°ì´ ì •í™•í•¨
            avg_resonance = statistics.mean(resonance_scores)
            
            # ì¤‘ìš”ë„ ì ìˆ˜ë„ ê³ ë ¤
            importance_scores = []
            for memory in recalled_memories:
                importance = memory.get("importance", 5000) / 10000.0
                importance_scores.append(importance)
            
            avg_importance = statistics.mean(importance_scores)
            
            # ì¢…í•© ì§ê° ì •í™•ë„
            intuition_accuracy = (avg_resonance * 0.7 + avg_importance * 0.3)
            return intuition_accuracy
            
        except Exception as e:
            logger.error(f"âŒ ì§ê° ì •í™•ë„ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def generate_test_report(self, test_summary: Dict[str, Any]) -> str:
        """í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            metrics = test_summary["metrics"]
            improvements = test_summary["improvements"]
            
            report = f"""
# ğŸ§  AURA ì‹œìŠ¤í…œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸

## ğŸ“Š í…ŒìŠ¤íŠ¸ ê°œìš”
- **í…ŒìŠ¤íŠ¸ ì¼ì‹œ**: {test_summary['timestamp']}
- **ì´ í…ŒìŠ¤íŠ¸ ìˆ˜**: {test_summary['total_tests']}ê°œ

## ğŸ¯ ì„±ëŠ¥ ì§€í‘œ

### ì²˜ë¦¬ ì„±ëŠ¥
- **í‰ê·  ì²˜ë¦¬ ì‹œê°„**: {metrics.avg_processing_time:.1f}ms
- **ê¸°ì–µ íšŒìƒ ì†ë„**: {metrics.memory_retrieval_speed:.1f}ms
- **í† í° íš¨ìœ¨ì„±**: {metrics.token_efficiency:.0f} í† í°

### í’ˆì§ˆ ì§€í‘œ
- **í‰ê·  ì‹ ë¢°ë„**: {metrics.avg_confidence_score:.3f} ({metrics.avg_confidence_score*100:.1f}%)
- **íšŒìƒ ì •í™•ë„**: {metrics.recall_accuracy:.3f} ({metrics.recall_accuracy*100:.1f}%)
- **í†µì°° í’ˆì§ˆ**: {metrics.insight_quality:.3f} ({metrics.insight_quality*100:.1f}%)
- **ì§€í˜œ íš¨ê³¼ì„±**: {metrics.wisdom_effectiveness:.3f} ({metrics.wisdom_effectiveness*100:.1f}%)
- **ì§„ë¦¬ íƒì§€ìœ¨**: {metrics.truth_detection_rate:.3f} ({metrics.truth_detection_rate*100:.1f}%)
- **ì¡´ì¬ ê°ê° ìˆ˜ì¤€**: {metrics.existence_sense_level:.3f} ({metrics.existence_sense_level*100:.1f}%)
- **ì§ê° ì •í™•ë„**: {metrics.intuition_accuracy:.3f} ({metrics.intuition_accuracy*100:.1f}%)

## ğŸ“ˆ ê°œì„ ë¥  (ê¸°ì¡´ ëŒ€ë¹„)

### ì„±ëŠ¥ ê°œì„ 
- **ì²˜ë¦¬ ì‹œê°„**: {improvements.get('processing_time_improvement', 0):+.1f}%
- **ê¸°ì–µ íšŒìƒ ì†ë„**: {improvements.get('memory_retrieval_speed_improvement', 0):+.1f}%
- **í† í° íš¨ìœ¨ì„±**: {improvements.get('token_efficiency_improvement', 0):+.1f}%

### í’ˆì§ˆ ê°œì„ 
- **ì‹ ë¢°ë„**: {improvements.get('confidence_improvement', 0):+.1f}%
- **íšŒìƒ ì •í™•ë„**: {improvements.get('recall_accuracy_improvement', 0):+.1f}%

## ğŸ† ê²°ë¡ 

AURA ì‹œìŠ¤í…œì€ ê¸°ì¡´ GPT ê¸°ë°˜ ì‹œìŠ¤í…œ ëŒ€ë¹„ ë‹¤ìŒê³¼ ê°™ì€ ê°œì„ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:

âœ… **ì²˜ë¦¬ ì†ë„**: {improvements.get('processing_time_improvement', 0):+.1f}% í–¥ìƒ
âœ… **í† í° íš¨ìœ¨ì„±**: {improvements.get('token_efficiency_improvement', 0):+.1f}% í–¥ìƒ  
âœ… **íšŒìƒ ì •í™•ë„**: {improvements.get('recall_accuracy_improvement', 0):+.1f}% í–¥ìƒ
âœ… **ì§ê° ì •í™•ë„**: {metrics.intuition_accuracy*100:.1f}% ë‹¬ì„±

ì´ëŠ” ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ëª©í‘œë¥¼ ëŒ€ë¶€ë¶„ ë‹¬ì„±í–ˆìŒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
"""
            
            return report
            
        except Exception as e:
            logger.error(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë¦¬í¬íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

def generate_test_cases() -> List[Dict[str, Any]]:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±"""
    return [
        {
            "description": "ì‚¶ì˜ ì˜ë¯¸ íƒêµ¬",
            "input": "ì‚¶ì˜ ì˜ë¯¸ì— ëŒ€í•´ ê¹Šì´ ìƒê°í•˜ê³  ìˆì–´ìš”. ë¬´ì—‡ì´ ì§„ì •ìœ¼ë¡œ ì¤‘ìš”í•œ ê²ƒì¼ê¹Œìš”?",
            "expected_tags": ["ì˜ë¯¸", "ìƒê°", "ì¤‘ìš”"],
            "expected_wisdom_level": "high",
            "context": {"user_id": "test_user_1", "session_id": "session_1"}
        },
        {
            "description": "ê¸°ìˆ ì  ë¬¸ì œ í•´ê²°",
            "input": "Pythonì—ì„œ ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "expected_tags": ["Python", "ë¹„ë™ê¸°", "êµ¬í˜„"],
            "expected_wisdom_level": "medium",
            "context": {"user_id": "test_user_2", "session_id": "session_2"}
        },
        {
            "description": "ê°ì •ì  ê³ ë¯¼ ìƒë‹´",
            "input": "ìµœê·¼ì— ì¸ê°„ê´€ê³„ì—ì„œ ì–´ë ¤ì›€ì„ ê²ªê³  ìˆì–´ìš”. ì–´ë–»ê²Œ ëŒ€ì²˜í•´ì•¼ í• ê¹Œìš”?",
            "expected_tags": ["ê´€ê³„", "ì–´ë ¤ì›€", "ëŒ€ì²˜"],
            "expected_wisdom_level": "high",
            "context": {"user_id": "test_user_3", "session_id": "session_3"}
        },
        {
            "description": "ì°½ì˜ì  ì•„ì´ë””ì–´ ë°œìƒ",
            "input": "ìƒˆë¡œìš´ ë¹„ì¦ˆë‹ˆìŠ¤ ì•„ì´ë””ì–´ë¥¼ êµ¬ìƒí•˜ê³  ìˆëŠ”ë°, ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì ‘ê·¼í•˜ë©´ ì¢‹ì„ê¹Œìš”?",
            "expected_tags": ["ì•„ì´ë””ì–´", "ë¹„ì¦ˆë‹ˆìŠ¤", "ì ‘ê·¼"],
            "expected_wisdom_level": "medium",
            "context": {"user_id": "test_user_4", "session_id": "session_4"}
        },
        {
            "description": "ì² í•™ì  ì§ˆë¬¸",
            "input": "ììœ ì˜ì§€ì™€ ê²°ì •ë¡ ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”? ì¸ê°„ì€ ì •ë§ ììœ ë¡œìš´ ì¡´ì¬ì¼ê¹Œìš”?",
            "expected_tags": ["ììœ ì˜ì§€", "ê²°ì •ë¡ ", "ì¸ê°„"],
            "expected_wisdom_level": "high",
            "context": {"user_id": "test_user_5", "session_id": "session_5"}
        }
    ]

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        # í…ŒìŠ¤í„° ì´ˆê¸°í™”
        tester = AURAPerformanceTester()
        await tester.initialize()
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
        test_cases = generate_test_cases()
        
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_summary = await tester.run_comprehensive_test(test_cases)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = tester.generate_test_report(test_summary)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        with open("aura_performance_report.md", "w", encoding="utf-8") as f:
            f.write(report)
        
        print("âœ… AURA ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("ğŸ“Š ê²°ê³¼ íŒŒì¼:")
        print("  - aura_performance_test_results.json")
        print("  - aura_performance_report.md")
        
        # ê°„ë‹¨í•œ ê²°ê³¼ ì¶œë ¥
        metrics = test_summary["metrics"]
        improvements = test_summary["improvements"]
        
        print(f"\nğŸ¯ ì£¼ìš” ê²°ê³¼:")
        print(f"  - í‰ê·  ì²˜ë¦¬ ì‹œê°„: {metrics.avg_processing_time:.1f}ms")
        print(f"  - í‰ê·  ì‹ ë¢°ë„: {metrics.avg_confidence_score*100:.1f}%")
        print(f"  - íšŒìƒ ì •í™•ë„: {metrics.recall_accuracy*100:.1f}%")
        print(f"  - ì§ê° ì •í™•ë„: {metrics.intuition_accuracy*100:.1f}%")
        print(f"  - ì²˜ë¦¬ ì‹œê°„ ê°œì„ : {improvements.get('processing_time_improvement', 0):+.1f}%")
        
    except Exception as e:
        logger.error(f"âŒ ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main()) 