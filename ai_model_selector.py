import os
import sys
import time
import openai
from dotenv import load_dotenv
from pathlib import Path
from openai import OpenAI
from tiktoken import encoding_for_model

# 1) .env íƒìƒ‰: í”„ë¡œì íŠ¸ ë£¨íŠ¸ -> src
script_dir = Path(__file__).resolve().parent
root_env = script_dir.parent / ".env"
src_env  = script_dir / ".env"
env_loaded = False  # âœ… Syntax ì˜¤ë¥˜ ìˆ˜ì •: ì—¬ê¸°ì„œ ì¤„ë°”ê¿ˆ ë¹ ì¡Œë˜ ë¶€ë¶„ ìˆ˜ì •

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for env_path in (root_env, src_env):
    if env_path.exists():
        try:
            load_dotenv(dotenv_path=env_path)
            print(f"ğŸ”„ Loaded .env from: {env_path}")
            env_loaded = True
            break
        except PermissionError as e:
            print(f"âš ï¸ .env íŒŒì¼ ì½ê¸° ê¶Œí•œ ì—†ìŒ: {env_path} ({e})", file=sys.stderr)
        except Exception as e:
            print(f"âš ï¸ .env ë¡œë“œ ì‹¤íŒ¨ ({env_path}): {e}", file=sys.stderr)

if not env_loaded:
    # Railway í™˜ê²½ì—ì„œëŠ” .env íŒŒì¼ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ
    is_railway = os.getenv("RAILWAY_ENVIRONMENT") is not None
    if not is_railway:
        print("âš ï¸ Warning: .env íŒŒì¼ì„ ì°¾ê±°ë‚˜ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.", file=sys.stderr)

# 2) API í‚¤ ë¡œë“œ
api_key = os.getenv("OPENAI_API_KEY", "").strip()
if not api_key:
    print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env ë˜ëŠ” ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.", file=sys.stderr)
    sys.exit(1)

# 3) í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai.api_key = api_key
client = OpenAI(
    api_key=api_key,
    # proxies ì¸ìˆ˜ ì œê±° - httpx 0.28.1 í˜¸í™˜ì„±
)  # âœ… OpenAI 1.7.0 ì´ìƒ ê¸°ì¤€ project_id ì œê±°

print("âœ… OpenAI API í‚¤ ë¡œë“œ ì™„ë£Œ")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìš”ì²­ ë©”íŠ¸ë¦­ ì¹´ìš´í„°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
request_counter = 0

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# GPT í˜¸ì¶œ í•¨ìˆ˜ (ìƒì„¸ ë¡œê¹… í¬í•¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# í† í° ê³„ì‚°ì„ ìœ„í•œ ì¸ì½”ë” ì´ˆê¸°í™”
enc = encoding_for_model("gpt-3.5-turbo")

def count_tokens(text: str) -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°"""
    try:
        return len(enc.encode(text))
    except Exception:
        return len(text.split()) * 1.3

def count_message_tokens(messages: list) -> int:
    """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì´ í† í° ìˆ˜ë¥¼ ê³„ì‚°"""
    total = 0
    for msg in messages:
        if isinstance(msg.get("content"), str):
            total += count_tokens(msg["content"])
    return total

def do_task(
    prompt=None,
    system_message=None,
    messages=None,
    model="gpt-4o",
    temperature=0.7,
    max_tokens=2048
):
    """
    GPT í˜¸ì¶œ í•¨ìˆ˜ (ìƒì„¸ ë¡œê¹… í¬í•¨)
    - prompt: ì‚¬ìš©ì ì…ë ¥ (None í—ˆìš©, messages ìˆì„ ë•Œ)
    - system_message: system ë©”ì‹œì§€
    - messages: ë¯¸ë¦¬ êµ¬ì„±ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    - model: ì‚¬ìš©í•  ëª¨ë¸
    """
    global request_counter
    request_counter += 1

    if not any([prompt, system_message, messages]):
        raise ValueError("do_task í˜¸ì¶œ ì‹œ prompt, system_message, messages ì¤‘ í•˜ë‚˜ëŠ” ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")

    if messages is None:
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        if prompt:
            messages.append({"role": "user", "content": prompt})

    # í† í° ìˆ˜ ê³„ì‚° ë° ì œí•œ
    total_tokens = count_message_tokens(messages)
    if total_tokens > 6000:  # ì•ˆì „ ë§ˆì§„ì„ ë‘ê³  ì œí•œ
        print(f"âš ï¸ ê²½ê³ : ë©”ì‹œì§€ í† í° ìˆ˜({total_tokens})ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. ì¼ë¶€ ë©”ì‹œì§€ê°€ ì œê±°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ìœ ì§€í•˜ê³  ë‚˜ë¨¸ì§€ ë©”ì‹œì§€ ì œí•œ
        system_msg = messages[0] if messages and messages[0]["role"] == "system" else None
        filtered_messages = [system_msg] if system_msg else []
        current_tokens = count_tokens(system_msg["content"]) if system_msg else 0
        
        for msg in messages[1:]:
            msg_tokens = count_tokens(msg["content"])
            if current_tokens + msg_tokens > 6000:
                break
            filtered_messages.append(msg)
            current_tokens += msg_tokens
        
        messages = filtered_messages

    start_time = time.time()
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens
    )
    elapsed = time.time() - start_time

    print(f"[Metrics] Request #{request_counter:<3} | "
          f"Model={model:<8} | Temp={temperature:<4} | "
          f"MaxTokens={max_tokens:<5} | "
          f"InputTokens={total_tokens:<5} | "
          f"Elapsed={elapsed:.3f}s")

    return response.choices[0].message.content

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë‹¨ìˆœ í˜¸ì¶œ ë²„ì „ (ì¤‘ë³µ ì •ì˜ ë³µì›)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def do_task(prompt=None, system_message=None, messages=None,
            model="gpt-4o", temperature=0.7, max_tokens=2048):
    """
    GPT í˜¸ì¶œ í•¨ìˆ˜
    - prompt: ì‚¬ìš©ì ì…ë ¥ (None í—ˆìš©, messages ìˆì„ ë•Œ)
    - system_message: system ë©”ì‹œì§€
    - messages: ë¯¸ë¦¬ êµ¬ì„±ëœ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    - model: ì‚¬ìš©í•  ëª¨ë¸
    """
    if not any([prompt, system_message, messages]):
        raise ValueError("do_task í˜¸ì¶œ ì‹œ prompt, system_message, messages ì¤‘ í•˜ë‚˜ëŠ” ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.")

    if messages is None:
        messages = []
        if system_message:
            messages.append({"role": "system", "content": system_message})
        if prompt:
            messages.append({"role": "user", "content": prompt})

    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens
    )
    return response.choices[0].message.content


import asyncio
async def do_task_async(
    prompt=None,
    system_message=None,
    messages=None,
    model="gpt-4o",
    temperature=0.7,
    max_tokens=2048
):
    return await asyncio.to_thread(
        do_task,
        prompt=prompt,
        system_message=system_message,
        messages=messages,
        model=model,
        temperature=temperature,
        max_tokens=max_tokens
    )
