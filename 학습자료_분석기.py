
"""
ê¸ˆê°•GPT í•™ìŠµìë£Œ ë¶„ì„ê¸°
- configs/ ë˜ëŠ” ./ai_brain/aiN í´ë” ë‚´ ì›Œë“œ, ì—‘ì…€, í…ìŠ¤íŠ¸, JSON ìë£Œ ìë™ ë¶„ì„
- í•™ìŠµ ë‚´ìš© â†’ DBí™” (memory_db.json)
- ê° AIì— í”„ë¡¬í”„íŠ¸ 5~7ê°œ ì €ì¥
- system_message_ê¸ˆê°•.txt ë“± í”„ë¡¬í”„íŠ¸ ìƒì„±
"""

import os
import json
import hashlib

try:
    import docx
    import openpyxl
except ImportError:
    print("â— docx/openpyxl ì„¤ì¹˜ í•„ìš”")

ROOT = "./configs"
AI_FOLDER = "./ai_brain"
DB_PATH = "memory_db.json"
PROMPT_PATH = "system_message_ê¸ˆê°•.txt"
AI_PROMPT_DB = "ai_prompts.json"

def flatten_json(obj, prefix=""):
    result = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            result.extend(flatten_json(v, f"{prefix}{k}: "))
    elif isinstance(obj, list):
        for i, v in enumerate(obj):
            result.extend(flatten_json(v, f"{prefix}[{i}] "))
    else:
        result.append(f"{prefix}{str(obj)}")
    return result

def hash_file(file):
    key = f"{file}:{os.path.getsize(file)}:{int(os.path.getmtime(file))}"
    return hashlib.md5(key.encode()).hexdigest()

def parse_file(path):
    ext = path.split(".")[-1].lower()
    results = []
    try:
        if ext == "docx":
            doc = docx.Document(path)
            results = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
        elif ext == "xlsx":
            wb = openpyxl.load_workbook(path)
            for sheet in wb.worksheets:
                for row in sheet.iter_rows(values_only=True):
                    for cell in row:
                        if cell:
                            results.append(str(cell))
        elif ext == "txt":
            with open(path, "r", encoding="utf-8") as f:
                results = [line.strip() for line in f if line.strip()]
        elif ext == "json":
            with open(path, "r", encoding="utf-8") as f:
                obj = json.load(f)
                results = flatten_json(obj)
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {path} - {e}")
    return results

def analyze_all(root=ROOT):
    db = {}
    seen = {}
    for base in [root, AI_FOLDER]:
        for dirpath, _, files in os.walk(base):
            for fname in files:
                if not fname.lower().endswith(("docx", "xlsx", "txt", "json")):
                    continue
                fpath = os.path.join(dirpath, fname)
                hashv = hash_file(fpath)
                if fname in seen and seen[fname] == hashv:
                    continue
                key = "ê¸ˆê°•" if "ê¸ˆê°•" in fpath else "ë ˆì¡°ë‚˜" if "ë ˆì¡°ë‚˜" in fpath else "ê¸°íƒ€"
                if key not in db:
                    db[key] = []
                parsed = parse_file(fpath)
                db[key].extend(parsed)
                seen[fname] = hashv
    with open(DB_PATH, "w", encoding="utf-8") as f:
        json.dump(db, f, indent=2)
    return db

def generate_system_prompt():
    db = json.load(open(DB_PATH, encoding="utf-8"))
    lines = db.get("ê¸ˆê°•", [])[:30]
    with open(PROMPT_PATH, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

def extract_ai_prompts():
    prompts = {}
    for ai_n in ["ai1", "ai2", "ai3", "ai4", "ai5"]:
        path = f"{AI_FOLDER}/{ai_n}"
        prompts[ai_n] = []
        if not os.path.exists(path):
            continue
        for fname in os.listdir(path):
            if not fname.endswith((".txt", ".docx")):
                continue
            lines = parse_file(os.path.join(path, fname))
            prompts[ai_n].extend(lines[:7])
    with open(AI_PROMPT_DB, "w", encoding="utf-8") as f:
        json.dump(prompts, f, indent=2)

if __name__ == "__main__":
    print("ğŸ“‚ ê¸ˆê°•GPT í•™ìŠµìë£Œ ë¶„ì„ ì¤‘...")
    analyze_all()
    generate_system_prompt()
    extract_ai_prompts()
    print("âœ… í•™ìŠµìë£Œ ë¶„ì„ + system_prompt ìƒì„± ì™„ë£Œ")
