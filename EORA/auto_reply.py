
from session_memory import update_context, get_context
from memory_db import save_chunk
from memory_loader import load_memory_chunks
from gpt_router import ask
import time

def auto_reply(user_input: str, session_id: str = "ai1", system_prompt: str = "", stream=False) -> str:
    recent_chunks = load_memory_chunks("ìµœê·¼ì‹œìŠ¤í…œê¸°ì–µ")
    recent_prompt = "\n".join(recent_chunks[:5]) if recent_chunks else ""

    if not system_prompt:
        system_prompt = get_context(session_id)

    # âœ… ë¶„ì„ ë‚´ìš© + ì§ˆë¬¸ì„ í•©ì¹œ user prompt ìƒì„±
    enhanced_prompt = f"""ì•„ë˜ëŠ” ì²¨ë¶€ëœ íŒŒì¼ì˜ ìë™ ë¶„ì„ ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°˜ë“œì‹œ ë°˜ì˜í•˜ì—¬ ëŒ€ë‹µí•˜ì„¸ìš”.

[ë¶„ì„ ìš”ì•½]
{recent_prompt}

[ì‚¬ìš©ìì˜ ì§ˆë¬¸]
{user_input}
"""

    print(f"ğŸ§  {session_id} system memory ì¤„ ìˆ˜: {len(system_prompt.splitlines())}")
    print(f"ğŸ§  {session_id} system memory ê¸¸ì´: {len(system_prompt)}ì")

    max_tokens = 512
    if len(user_input) > 800 or len(recent_prompt) > 2000:
        max_tokens = 400
    elif len(user_input) > 1500:
        max_tokens = 300

    reply = ask(
        prompt=enhanced_prompt,
        system_msg=system_prompt,
        stream=stream,
        max_tokens=max_tokens
    )

    print("[DEBUG] GPT ì‘ë‹µ ë‚´ìš©:", reply[:300])
    print("[DEBUG] ì‘ë‹µ ê¸¸ì´:", len(reply))

    if reply:
        save_chunk("ëŒ€í™”í•™ìŠµ", reply)

    update_context(session_id, system_prompt)
    return reply if reply else "ğŸ¤– GPTê°€ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
