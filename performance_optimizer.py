#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EORA AI ì„±ëŠ¥ ìµœì í™” ëª¨ë“ˆ
API ì‘ë‹µì†ë„ í–¥ìƒì„ ìœ„í•œ ìµœì í™” ì‹œìŠ¤í…œ
"""

import time
import asyncio
import threading
from typing import Dict, Any, List, Optional, Callable
from functools import wraps, lru_cache
import logging
from datetime import datetime, timedelta
import json
import hashlib

logger = logging.getLogger(__name__)


class PerformanceOptimizer:
    """ì„±ëŠ¥ ìµœì í™” ë° ìºì‹± ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.response_cache = {}
        self.memory_cache = {}
        self.session_cache = {}
        self.cache_ttl = 300  # 5ë¶„ ìºì‹œ
        self.max_cache_size = 1000
        self.performance_stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'avg_response_time': 0.0,
            'slow_requests': 0
        }
        
    def cache_key(self, *args, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = str(args) + str(sorted(kwargs.items()))
        return hashlib.md5(key_data.encode()).hexdigest()
    
    def is_cache_valid(self, timestamp: datetime) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        return (datetime.now() - timestamp).seconds < self.cache_ttl
    
    def cache_response(self, key: str, data: Any) -> None:
        """ì‘ë‹µ ìºì‹±"""
        if len(self.response_cache) >= self.max_cache_size:
            # ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì œê±°
            oldest_key = min(self.response_cache.keys(), 
                           key=lambda k: self.response_cache[k]['timestamp'])
            del self.response_cache[oldest_key]
        
        self.response_cache[key] = {
            'data': data,
            'timestamp': datetime.now()
        }
    
    def get_cached_response(self, key: str) -> Optional[Any]:
        """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
        if key in self.response_cache:
            cache_item = self.response_cache[key]
            if self.is_cache_valid(cache_item['timestamp']):
                self.performance_stats['cache_hits'] += 1
                return cache_item['data']
            else:
                del self.response_cache[key]
        return None


def performance_monitor(func):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            return result
        finally:
            end_time = time.time()
            response_time = end_time - start_time
            
            # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
            optimizer.performance_stats['total_requests'] += 1
            current_avg = optimizer.performance_stats['avg_response_time']
            total_requests = optimizer.performance_stats['total_requests']
            
            optimizer.performance_stats['avg_response_time'] = (
                (current_avg * (total_requests - 1) + response_time) / total_requests
            )
            
            if response_time > 2.0:  # 2ì´ˆ ì´ìƒì´ë©´ ëŠë¦° ìš”ì²­
                optimizer.performance_stats['slow_requests'] += 1
                logger.warning(f"âš ï¸ ëŠë¦° ì‘ë‹µ ê°ì§€: {func.__name__} - {response_time:.2f}ì´ˆ")
            
            logger.info(f"âš¡ {func.__name__}: {response_time:.3f}ì´ˆ")
    
    return wrapper


def cached_response(ttl: int = 300):
    """ì‘ë‹µ ìºì‹± ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = optimizer.cache_key(func.__name__, *args, **kwargs)
            
            # ìºì‹œëœ ì‘ë‹µ í™•ì¸
            cached = optimizer.get_cached_response(cache_key)
            if cached:
                logger.info(f"ğŸ’¾ ìºì‹œ íˆíŠ¸: {func.__name__}")
                return cached
            
            # ìƒˆ ì‘ë‹µ ìƒì„± ë° ìºì‹±
            result = await func(*args, **kwargs)
            optimizer.cache_response(cache_key, result)
            
            return result
        
        return wrapper
    return decorator


class AsyncBatchProcessor:
    """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, batch_size: int = 10, max_wait_time: float = 0.1):
        self.batch_size = batch_size
        self.max_wait_time = max_wait_time
        self.pending_requests = []
        self.batch_lock = asyncio.Lock()
        
    async def add_to_batch(self, request_data: Dict[str, Any]) -> Any:
        """ë°°ì¹˜ì— ìš”ì²­ ì¶”ê°€"""
        async with self.batch_lock:
            future = asyncio.Future()
            self.pending_requests.append({
                'data': request_data,
                'future': future
            })
            
            # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ë˜ëŠ” ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ ì‹œ ì²˜ë¦¬
            if len(self.pending_requests) >= self.batch_size:
                await self._process_batch()
            else:
                asyncio.create_task(self._wait_and_process())
            
            return await future
    
    async def _wait_and_process(self):
        """ëŒ€ê¸° í›„ ë°°ì¹˜ ì²˜ë¦¬"""
        await asyncio.sleep(self.max_wait_time)
        async with self.batch_lock:
            if self.pending_requests:
                await self._process_batch()
    
    async def _process_batch(self):
        """ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰"""
        if not self.pending_requests:
            return
        
        current_batch = self.pending_requests.copy()
        self.pending_requests.clear()
        
        try:
            # ë°°ì¹˜ ì²˜ë¦¬ ë¡œì§ (ë³‘ë ¬ ì²˜ë¦¬)
            tasks = []
            for request in current_batch:
                task = self._process_single_request(request['data'])
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ë°˜í™˜
            for request, result in zip(current_batch, results):
                if isinstance(result, Exception):
                    request['future'].set_exception(result)
                else:
                    request['future'].set_result(result)
                    
        except Exception as e:
            # ëª¨ë“  ìš”ì²­ì— ì—ëŸ¬ ë°˜í™˜
            for request in current_batch:
                request['future'].set_exception(e)
    
    async def _process_single_request(self, request_data: Dict[str, Any]) -> Any:
        """ë‹¨ì¼ ìš”ì²­ ì²˜ë¦¬"""
        # ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ì€ ìƒì†ë°›ëŠ” í´ë˜ìŠ¤ì—ì„œ êµ¬í˜„
        return request_data


class FastMemoryRecall:
    """ë¹ ë¥¸ ë©”ëª¨ë¦¬ íšŒìƒ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.keyword_index = {}
        self.embedding_cache = {}
        self.recent_queries = {}
        
    def build_keyword_index(self, memories: List[Dict[str, Any]]) -> None:
        """í‚¤ì›Œë“œ ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self.keyword_index.clear()
        
        for memory in memories:
            content = memory.get('content', '').lower()
            memory_id = memory.get('memory_id') or memory.get('_id')
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì¸ë±ì‹±
            words = content.split()
            for word in words:
                if len(word) > 2:  # 2ê¸€ì ì´ìƒë§Œ ì¸ë±ì‹±
                    if word not in self.keyword_index:
                        self.keyword_index[word] = []
                    self.keyword_index[word].append(memory_id)
    
    def fast_keyword_search(self, query: str, limit: int = 10) -> List[str]:
        """ë¹ ë¥¸ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        query_words = query.lower().split()
        memory_scores = {}
        
        for word in query_words:
            if word in self.keyword_index:
                for memory_id in self.keyword_index[word]:
                    memory_scores[memory_id] = memory_scores.get(memory_id, 0) + 1
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_memories = sorted(memory_scores.items(), key=lambda x: x[1], reverse=True)
        return [memory_id for memory_id, _ in sorted_memories[:limit]]


class DatabaseOptimizer:
    """ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”"""
    
    def __init__(self):
        self.connection_pool = None
        self.query_cache = {}
        self.prepared_statements = {}
    
    async def optimize_mongodb_queries(self, collection):
        """MongoDB ì¿¼ë¦¬ ìµœì í™”"""
        try:
            # ì¸ë±ìŠ¤ ìƒì„±
            await collection.create_index([("user_id", 1), ("timestamp", -1)])
            await collection.create_index([("content", "text")])
            await collection.create_index([("memory_type", 1), ("user_id", 1)])
            
            logger.info("âœ… MongoDB ì¸ë±ìŠ¤ ìµœì í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ MongoDB ìµœì í™” ì‹¤íŒ¨: {e}")
    
    def optimize_query(self, query: Dict[str, Any]) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ìµœì í™”"""
        optimized = query.copy()
        
        # ë¶ˆí•„ìš”í•œ í•„ë“œ ì œê±°
        if 'projection' not in optimized:
            optimized['projection'] = {
                'content': 1,
                'timestamp': 1,
                'user_id': 1,
                'memory_type': 1
            }
        
        # ì œí•œ ì„¤ì •
        if 'limit' not in optimized:
            optimized['limit'] = 50
        
        return optimized


class ResponseCompressor:
    """ì‘ë‹µ ì••ì¶• ì‹œìŠ¤í…œ"""
    
    @staticmethod
    def compress_response(data: Dict[str, Any]) -> Dict[str, Any]:
        """ì‘ë‹µ ë°ì´í„° ì••ì¶•"""
        compressed = {}
        
        # í•„ìˆ˜ í•„ë“œë§Œ í¬í•¨
        essential_fields = ['success', 'response', 'formatted_response', 'session_id']
        for field in essential_fields:
            if field in data:
                compressed[field] = data[field]
        
        # ê¸´ í…ìŠ¤íŠ¸ ìµœì í™”
        if 'response' in compressed and len(compressed['response']) > 1000:
            # ë§ˆí¬ë‹¤ìš´ ì²˜ë¦¬ëœ ë²„ì „ì´ ìˆìœ¼ë©´ ì›ë³¸ì€ ìš”ì•½
            if 'formatted_response' in compressed:
                compressed['response'] = compressed['response'][:500] + "..."
        
        return compressed


# ì „ì—­ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
optimizer = PerformanceOptimizer()
batch_processor = AsyncBatchProcessor()
fast_recall = FastMemoryRecall()
db_optimizer = DatabaseOptimizer()
compressor = ResponseCompressor()


async def optimize_chat_response(original_func):
    """ì±„íŒ… ì‘ë‹µ ìµœì í™” ë˜í¼"""
    @wraps(original_func)
    @performance_monitor
    @cached_response(ttl=60)  # 1ë¶„ ìºì‹œ
    async def optimized_wrapper(*args, **kwargs):
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìš”ì²­ ìµœì í™”
        request_data = {
            'func': original_func,
            'args': args,
            'kwargs': kwargs
        }
        
        result = await batch_processor.add_to_batch(request_data)
        
        # ì‘ë‹µ ì••ì¶•
        if isinstance(result, dict):
            result = compressor.compress_response(result)
        
        return result
    
    return optimized_wrapper


def get_performance_stats() -> Dict[str, Any]:
    """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
    stats = optimizer.performance_stats.copy()
    stats.update({
        'cache_size': len(optimizer.response_cache),
        'cache_hit_rate': (
            stats['cache_hits'] / max(stats['total_requests'], 1) * 100
        ),
        'slow_request_rate': (
            stats['slow_requests'] / max(stats['total_requests'], 1) * 100
        )
    })
    return stats


async def warm_up_system():
    """ì‹œìŠ¤í…œ ì›Œë°ì—…"""
    logger.info("ğŸ”¥ ì‹œìŠ¤í…œ ì›Œë°ì—… ì‹œì‘...")
    
    try:
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìµœì í™”
        from database import db_manager
        db_mgr = db_manager()
        if db_mgr and hasattr(db_mgr, 'memories_collection'):
            await db_optimizer.optimize_mongodb_queries(db_mgr.memories_collection)
        
        # ë©”ëª¨ë¦¬ ì¸ë±ìŠ¤ êµ¬ì¶•
        # (ì‹¤ì œ ë©”ëª¨ë¦¬ ë°ì´í„°ê°€ ìˆì„ ë•Œ êµ¬ì¶•)
        
        logger.info("âœ… ì‹œìŠ¤í…œ ì›Œë°ì—… ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì‹œìŠ¤í…œ ì›Œë°ì—… ì‹¤íŒ¨: {e}")


# ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ ì›Œë°ì—…
async def initialize_optimizer():
    """ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    try:
        await warm_up_system()
        print("âœ… ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ì‹œìŠ¤í…œì´ ì¤‘ë‹¨ë˜ì§€ ì•Šë„ë¡ í•¨ 