#!/usr/bin/env python3
"""
í•™ìŠµ ë°ì´í„° íë¦„ ì •ë°€ ë¶„ì„ê¸°
- ì €ì¥ê³¼ ë¶ˆëŸ¬ì˜¤ê¸° ê³¼ì •ì˜ ê° ë‹¨ê³„ë¥¼ ìƒì„¸íˆ ì¶”ì 
- ë°ì´í„° ë³€í™˜ ê³¼ì • ë¶„ì„
- í•„ë“œ ë§¤í•‘ ë¬¸ì œ ì§„ë‹¨
"""

import asyncio
import sys
import os
import json
from datetime import datetime
from typing import Dict, List, Optional, Any
import logging

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ íŒŒì´ì¬ ê²½ë¡œì— ì¶”ê°€
sys.path.append('.')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LearningDataFlowAnalyzer:
    """í•™ìŠµ ë°ì´í„° íë¦„ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "storage_analysis": {},
            "retrieval_analysis": {},
            "field_mapping": {},
            "data_transformation": {},
            "compatibility_issues": []
        }
    
    async def analyze_complete_flow(self) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„° íë¦„ ë¶„ì„"""
        logger.info("ğŸ”¬ í•™ìŠµ ë°ì´í„° íë¦„ ì •ë°€ ë¶„ì„ ì‹œì‘")
        logger.info("=" * 60)
        
        try:
            # 1. ì €ì¥ ê³¼ì • ë¶„ì„
            await self._analyze_storage_process()
            
            # 2. ì €ì¥ëœ ë°ì´í„° êµ¬ì¡° ë¶„ì„
            await self._analyze_stored_data_structure()
            
            # 3. ê²€ìƒ‰ ì¿¼ë¦¬ ë¶„ì„
            await self._analyze_search_queries()
            
            # 4. í•„ë“œ ë§¤í•‘ ë¬¸ì œ ì§„ë‹¨
            await self._diagnose_field_mapping()
            
            # 5. í˜¸í™˜ì„± ë¬¸ì œ ë¶„ì„
            await self._analyze_compatibility_issues()
            
            # 6. ìµœì¢… ê¶Œì¥ì‚¬í•­ ìƒì„±
            self._generate_recommendations()
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        
        return self.analysis_results
    
    async def _analyze_storage_process(self):
        """ì €ì¥ ê³¼ì • ë¶„ì„"""
        logger.info("1ï¸âƒ£ ì €ì¥ ê³¼ì • ë¶„ì„")
        
        try:
            from mongodb_config import get_optimized_mongodb_connection
            from enhanced_learning_system import get_enhanced_learning_system
            
            client = get_optimized_mongodb_connection()
            learning_system = get_enhanced_learning_system(client)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°
            test_content = "ì´ê²ƒì€ ë°ì´í„° íë¦„ ë¶„ì„ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ë‚´ìš©ì…ë‹ˆë‹¤. Python í”„ë¡œê·¸ë˜ë°ê³¼ ì¸ê³µì§€ëŠ¥ì— ëŒ€í•œ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤."
            test_filename = "dataflow_test.txt"
            test_category = "ë¶„ì„í…ŒìŠ¤íŠ¸"
            
            logger.info(f"ğŸ“ í…ŒìŠ¤íŠ¸ ì €ì¥ ì‹œì‘: {test_filename}")
            
            # ì €ì¥ ì „ ìƒíƒœ í™•ì¸
            db = learning_system.db
            before_count = db.memories.count_documents({"memory_type": "enhanced_learning"})
            
            # ì €ì¥ ì‹¤í–‰
            storage_result = await learning_system.learn_document(
                content=test_content,
                filename=test_filename,
                category=test_category
            )
            
            # ì €ì¥ í›„ ìƒíƒœ í™•ì¸
            after_count = db.memories.count_documents({"memory_type": "enhanced_learning"})
            
            # ì €ì¥ëœ ë°ì´í„° í™•ì¸
            stored_memories = list(db.memories.find({
                "source_file": test_filename,
                "memory_type": "enhanced_learning"
            }))
            
            self.analysis_results["storage_analysis"] = {
                "success": storage_result.get("success", False),
                "error": storage_result.get("error"),
                "before_count": before_count,
                "after_count": after_count,
                "new_memories": len(stored_memories),
                "expected_chunks": storage_result.get("total_chunks", 0),
                "saved_memory_ids": storage_result.get("saved_memories", []),
                "storage_details": storage_result
            }
            
            if stored_memories:
                # ì²« ë²ˆì§¸ ì €ì¥ëœ ë©”ëª¨ë¦¬ì˜ êµ¬ì¡° ë¶„ì„
                sample_memory = stored_memories[0]
                logger.info("ğŸ“Š ì €ì¥ëœ ë©”ëª¨ë¦¬ êµ¬ì¡° ë¶„ì„:")
                
                for key, value in sample_memory.items():
                    if key != "_id":
                        value_type = type(value).__name__
                        value_preview = str(value)[:50] if len(str(value)) > 50 else str(value)
                        logger.info(f"   ğŸ”¸ {key} ({value_type}): {value_preview}")
                
                self.analysis_results["storage_analysis"]["sample_structure"] = {
                    k: type(v).__name__ for k, v in sample_memory.items()
                }
            
            logger.info(f"âœ… ì €ì¥ ë¶„ì„ ì™„ë£Œ - {before_count} â†’ {after_count} (+{after_count - before_count})")
            
        except Exception as e:
            logger.error(f"âŒ ì €ì¥ ê³¼ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            self.analysis_results["storage_analysis"]["error"] = str(e)
    
    async def _analyze_stored_data_structure(self):
        """ì €ì¥ëœ ë°ì´í„° êµ¬ì¡° ë¶„ì„"""
        logger.info("\n2ï¸âƒ£ ì €ì¥ëœ ë°ì´í„° êµ¬ì¡° ë¶„ì„")
        
        try:
            from mongodb_config import get_optimized_database
            
            db = get_optimized_database()
            memories = db.memories
            
            # ë©”ëª¨ë¦¬ íƒ€ì…ë³„ í•„ë“œ ë¶„ì„
            memory_types = ["enhanced_learning", "document_chunk"]
            
            for memory_type in memory_types:
                logger.info(f"ğŸ“‹ {memory_type} íƒ€ì… ë¶„ì„:")
                
                # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ
                sample = memories.find_one({"memory_type": memory_type})
                
                if sample:
                    fields = {}
                    for key, value in sample.items():
                        if key != "_id":
                            fields[key] = {
                                "type": type(value).__name__,
                                "exists": True,
                                "sample_value": str(value)[:30] if value else None
                            }
                    
                    self.analysis_results["field_mapping"][memory_type] = fields
                    
                    # ì£¼ìš” í•„ë“œ í™•ì¸
                    important_fields = ["content", "response", "filename", "source_file", "category", "keywords", "tags"]
                    for field in important_fields:
                        exists = field in fields
                        logger.info(f"   {'âœ…' if exists else 'âŒ'} {field}: {'ì¡´ì¬' if exists else 'ì—†ìŒ'}")
                else:
                    logger.info(f"   âš ï¸ {memory_type} íƒ€ì…ì˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
                    self.analysis_results["field_mapping"][memory_type] = {}
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    async def _analyze_search_queries(self):
        """ê²€ìƒ‰ ì¿¼ë¦¬ ë¶„ì„"""
        logger.info("\n3ï¸âƒ£ ê²€ìƒ‰ ì¿¼ë¦¬ ë¶„ì„")
        
        try:
            from eora_memory_system import EORAMemorySystem
            
            memory_system = EORAMemorySystem()
            
            test_queries = [
                {"query": "ë¶„ì„í…ŒìŠ¤íŠ¸", "memory_type": "enhanced_learning"},
                {"query": "Python", "memory_type": "enhanced_learning"},
                {"query": "dataflow", "memory_type": "enhanced_learning"},
                {"query": "ì¸ê³µì§€ëŠ¥", "memory_type": None}  # íƒ€ì… ì œí•œ ì—†ìŒ
            ]
            
            search_results = {}
            
            for test_case in test_queries:
                query = test_case["query"]
                memory_type = test_case["memory_type"]
                
                logger.info(f"ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}' (íƒ€ì…: {memory_type or 'ì „ì²´'})")
                
                try:
                    results = await memory_system.recall_learned_content(
                        query=query,
                        memory_type=memory_type,
                        limit=10
                    )
                    
                    search_results[f"{query}_{memory_type or 'all'}"] = {
                        "query": query,
                        "memory_type": memory_type,
                        "result_count": len(results),
                        "results": []
                    }
                    
                    logger.info(f"   ğŸ“Š ê²°ê³¼: {len(results)}ê°œ")
                    
                    # ê²°ê³¼ ìƒì„¸ ë¶„ì„
                    for i, result in enumerate(results[:3]):  # ìµœëŒ€ 3ê°œ
                        result_analysis = {
                            "memory_type": result.get("memory_type"),
                            "has_content": "content" in result,
                            "has_response": "response" in result,
                            "has_filename": "filename" in result or "source_file" in result,
                            "has_category": "category" in result,
                            "relevance_score": result.get("relevance_score", 0)
                        }
                        
                        search_results[f"{query}_{memory_type or 'all'}"]["results"].append(result_analysis)
                        
                        content_field = "content" if "content" in result else "response"
                        content = result.get(content_field, "")[:40]
                        filename = result.get("filename", result.get("source_file", "unknown"))
                        
                        logger.info(f"     ğŸ“„ ê²°ê³¼ {i+1}: {filename} - {content}...")
                
                except Exception as e:
                    logger.error(f"   âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
                    search_results[f"{query}_{memory_type or 'all'}"] = {"error": str(e)}
            
            self.analysis_results["retrieval_analysis"] = search_results
            
        except Exception as e:
            logger.error(f"âŒ ê²€ìƒ‰ ì¿¼ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    async def _diagnose_field_mapping(self):
        """í•„ë“œ ë§¤í•‘ ë¬¸ì œ ì§„ë‹¨"""
        logger.info("\n4ï¸âƒ£ í•„ë“œ ë§¤í•‘ ë¬¸ì œ ì§„ë‹¨")
        
        try:
            # enhanced_learningê³¼ document_chunk ê°„ í•„ë“œ ë¹„êµ
            enhanced_fields = self.analysis_results["field_mapping"].get("enhanced_learning", {})
            document_fields = self.analysis_results["field_mapping"].get("document_chunk", {})
            
            # í•„ë“œ ë§¤í•‘ í…Œì´ë¸”
            field_mappings = {
                "content_field": {
                    "enhanced_learning": "response",
                    "document_chunk": "content",
                    "compatible": True
                },
                "filename_field": {
                    "enhanced_learning": "source_file",
                    "document_chunk": "filename",
                    "compatible": True
                },
                "keywords_field": {
                    "enhanced_learning": "tags",
                    "document_chunk": "keywords",
                    "compatible": True
                },
                "category_field": {
                    "enhanced_learning": "category",
                    "document_chunk": "category",
                    "compatible": True
                }
            }
            
            mapping_issues = []
            
            for mapping_name, mapping_info in field_mappings.items():
                enhanced_field = mapping_info["enhanced_learning"]
                document_field = mapping_info["document_chunk"]
                
                enhanced_exists = enhanced_field in enhanced_fields
                document_exists = document_field in document_fields
                
                logger.info(f"ğŸ”— {mapping_name}:")
                logger.info(f"   enhanced_learning.{enhanced_field}: {'âœ…' if enhanced_exists else 'âŒ'}")
                logger.info(f"   document_chunk.{document_field}: {'âœ…' if document_exists else 'âŒ'}")
                
                if not enhanced_exists or not document_exists:
                    issue = f"{mapping_name}: í•„ë“œ ëˆ„ë½ - enhanced:{enhanced_exists}, document:{document_exists}"
                    mapping_issues.append(issue)
                    self.analysis_results["compatibility_issues"].append(issue)
            
            self.analysis_results["data_transformation"]["field_mappings"] = field_mappings
            self.analysis_results["data_transformation"]["mapping_issues"] = mapping_issues
            
            if not mapping_issues:
                logger.info("âœ… í•„ë“œ ë§¤í•‘ì— ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤")
            else:
                logger.warning(f"âš ï¸ {len(mapping_issues)}ê°œì˜ í•„ë“œ ë§¤í•‘ ë¬¸ì œ ë°œê²¬")
            
        except Exception as e:
            logger.error(f"âŒ í•„ë“œ ë§¤í•‘ ì§„ë‹¨ ì‹¤íŒ¨: {e}")
    
    async def _analyze_compatibility_issues(self):
        """í˜¸í™˜ì„± ë¬¸ì œ ë¶„ì„"""
        logger.info("\n5ï¸âƒ£ í˜¸í™˜ì„± ë¬¸ì œ ë¶„ì„")
        
        try:
            # ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ì—ì„œ í˜¸í™˜ì„± ë¬¸ì œ ì°¾ê¸°
            retrieval_analysis = self.analysis_results["retrieval_analysis"]
            
            for search_key, search_data in retrieval_analysis.items():
                if "error" in search_data:
                    continue
                
                query = search_data.get("query")
                result_count = search_data.get("result_count", 0)
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì›ì¸ ë¶„ì„
                if result_count == 0:
                    issue = f"ê²€ìƒ‰ì–´ '{query}' ê²°ê³¼ ì—†ìŒ - í•„ë“œ ë§¤í•‘ ë˜ëŠ” ë°ì´í„° ì €ì¥ ë¬¸ì œ ê°€ëŠ¥ì„±"
                    self.analysis_results["compatibility_issues"].append(issue)
                    logger.warning(f"âš ï¸ {issue}")
                
                # ê²°ê³¼ëŠ” ìˆì§€ë§Œ ê´€ë ¨ì„±ì´ ë‚®ì€ ê²½ìš°
                elif result_count > 0:
                    results = search_data.get("results", [])
                    if results:
                        avg_relevance = sum(r.get("relevance_score", 0) for r in results) / len(results)
                        if avg_relevance < 1.0:
                            issue = f"ê²€ìƒ‰ì–´ '{query}' ê´€ë ¨ì„± ì ìˆ˜ ë‚®ìŒ ({avg_relevance:.2f}) - ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ ê°œì„  í•„ìš”"
                            self.analysis_results["compatibility_issues"].append(issue)
                            logger.warning(f"âš ï¸ {issue}")
            
            # ë©”ëª¨ë¦¬ íƒ€ì…ë³„ ì¼ê´€ì„± ê²€ì‚¬
            storage_analysis = self.analysis_results["storage_analysis"]
            expected_chunks = storage_analysis.get("expected_chunks", 0)
            new_memories = storage_analysis.get("new_memories", 0)
            
            if expected_chunks != new_memories:
                issue = f"ì €ì¥ ë¶ˆì¼ì¹˜ - ì˜ˆìƒ ì²­í¬: {expected_chunks}, ì‹¤ì œ ì €ì¥: {new_memories}"
                self.analysis_results["compatibility_issues"].append(issue)
                logger.warning(f"âš ï¸ {issue}")
            
        except Exception as e:
            logger.error(f"âŒ í˜¸í™˜ì„± ë¬¸ì œ ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def _generate_recommendations(self):
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        logger.info("\n6ï¸âƒ£ ê¶Œì¥ì‚¬í•­ ìƒì„±")
        
        recommendations = []
        
        # í˜¸í™˜ì„± ë¬¸ì œ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        issues = self.analysis_results["compatibility_issues"]
        
        if any("í•„ë“œ ëˆ„ë½" in issue for issue in issues):
            recommendations.append({
                "priority": "HIGH",
                "category": "í•„ë“œ ë§¤í•‘",
                "description": "EORA ë©”ëª¨ë¦¬ ì‹œìŠ¤í…œì˜ ê²€ìƒ‰ ì¿¼ë¦¬ì— enhanced_learning í•„ë“œ ì¶”ê°€ í•„ìš”",
                "action": "eora_memory_system.pyì˜ recall_learned_content í•¨ìˆ˜ì—ì„œ response, source_file, tags í•„ë“œ ê²€ìƒ‰ ì¡°ê±´ ì¶”ê°€"
            })
        
        if any("ê²°ê³¼ ì—†ìŒ" in issue for issue in issues):
            recommendations.append({
                "priority": "HIGH",
                "category": "ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜",
                "description": "ì €ì¥ëœ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•˜ëŠ” ë¬¸ì œ í•´ê²° í•„ìš”",
                "action": "ê²€ìƒ‰ ì¿¼ë¦¬ì˜ í•„ë“œëª…ê³¼ ì‹¤ì œ ì €ì¥ëœ í•„ë“œëª… ì¼ì¹˜ í™•ì¸"
            })
        
        if any("ê´€ë ¨ì„± ì ìˆ˜ ë‚®ìŒ" in issue for issue in issues):
            recommendations.append({
                "priority": "MEDIUM",
                "category": "ê²€ìƒ‰ í’ˆì§ˆ",
                "description": "ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„± í–¥ìƒ í•„ìš”",
                "action": "ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜ ê°œì„  ë° ê°€ì¤‘ì¹˜ ì¡°ì •"
            })
        
        if any("ì €ì¥ ë¶ˆì¼ì¹˜" in issue for issue in issues):
            recommendations.append({
                "priority": "HIGH",
                "category": "ë°ì´í„° ì¼ê´€ì„±",
                "description": "ì €ì¥ ê³¼ì •ì—ì„œ ë°ì´í„° ì†ì‹¤ ë°œìƒ",
                "action": "enhanced_learning_system.pyì˜ ì €ì¥ ë¡œì§ ì ê²€ ë° íŠ¸ëœì­ì…˜ ì²˜ë¦¬ ê°•í™”"
            })
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
        if not issues:
            recommendations.append({
                "priority": "LOW",
                "category": "ìµœì í™”",
                "description": "í˜„ì¬ ì‹œìŠ¤í…œì´ ì •ìƒ ì‘ë™ ì¤‘ì´ë©° ì¶”ê°€ ìµœì í™” ê°€ëŠ¥",
                "action": "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì •ê¸°ì ì¸ ì¸ë±ìŠ¤ ìµœì í™”"
            })
        
        self.analysis_results["recommendations"] = recommendations
        
        logger.info("ğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(recommendations, 1):
            logger.info(f"   {i}. [{rec['priority']}] {rec['category']}: {rec['description']}")
            logger.info(f"      â†’ {rec['action']}")
    
    def print_summary(self):
        """ìš”ì•½ ë³´ê³ ì„œ ì¶œë ¥"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ í•™ìŠµ ë°ì´í„° íë¦„ ë¶„ì„ ìš”ì•½")
        logger.info("=" * 60)
        
        # ì €ì¥ ìƒíƒœ
        storage = self.analysis_results["storage_analysis"]
        if storage.get("success"):
            logger.info(f"âœ… ì €ì¥: ì„±ê³µ ({storage.get('new_memories', 0)}ê°œ ë©”ëª¨ë¦¬)")
        else:
            logger.info(f"âŒ ì €ì¥: ì‹¤íŒ¨ - {storage.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
        
        # ê²€ìƒ‰ ìƒíƒœ
        retrieval = self.analysis_results["retrieval_analysis"]
        successful_searches = sum(1 for v in retrieval.values() if isinstance(v, dict) and v.get("result_count", 0) > 0)
        total_searches = len(retrieval)
        logger.info(f"ğŸ” ê²€ìƒ‰: {successful_searches}/{total_searches}ê°œ ì„±ê³µ")
        
        # í˜¸í™˜ì„± ë¬¸ì œ
        issues_count = len(self.analysis_results["compatibility_issues"])
        if issues_count == 0:
            logger.info("âœ… í˜¸í™˜ì„±: ë¬¸ì œ ì—†ìŒ")
        else:
            logger.info(f"âš ï¸ í˜¸í™˜ì„±: {issues_count}ê°œ ë¬¸ì œ ë°œê²¬")
        
        # ê¶Œì¥ì‚¬í•­
        recommendations = self.analysis_results["recommendations"]
        high_priority = sum(1 for r in recommendations if r["priority"] == "HIGH")
        logger.info(f"ğŸ’¡ ê¶Œì¥ì‚¬í•­: {len(recommendations)}ê°œ (ê¸´ê¸‰: {high_priority}ê°œ)")
        
        logger.info("=" * 60)

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = LearningDataFlowAnalyzer()
    
    try:
        results = await analyzer.analyze_complete_flow()
        analyzer.print_summary()
        
        # ìƒì„¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        output_file = f"learning_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"\nğŸ“„ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ {output_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì¢…ë£Œ ì½”ë“œ ê²°ì •
        critical_issues = sum(1 for r in results["recommendations"] if r["priority"] == "HIGH")
        if critical_issues == 0:
            sys.exit(0)  # ì„±ê³µ
        else:
            sys.exit(1)  # ë¬¸ì œ ë°œê²¬
        
    except Exception as e:
        logger.error(f"âŒ ë¶„ì„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        sys.exit(2)

if __name__ == "__main__":
    print("ğŸ”¬ í•™ìŠµ ë°ì´í„° íë¦„ ì •ë°€ ë¶„ì„ê¸°")
    print("ì´ ë„êµ¬ëŠ” í•™ìŠµ ë°ì´í„°ì˜ ì €ì¥ê³¼ ë¶ˆëŸ¬ì˜¤ê¸° ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.")
    print()
    
    asyncio.run(main())