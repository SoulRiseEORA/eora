"""
hybrid_recall_manager.py

ğŸ§  ë‹¤ì¤‘ íšŒìƒ ì „ëµ ìë™ íŒë‹¨ ë° ìš°ì„ ìˆœìœ„ ë³‘ë ¬ ì ìš© ëª¨ë“ˆ
- ì •ê·œ íšŒìƒ (íƒœê·¸, ë²¡í„°)
- ë§ê° ê¸°ë°˜ í•„í„°
- ë°˜ì‚¬ì  1íšŒì„± íšŒìƒ
- ìœ ì‚¬ íšŒìƒ ìƒì„± ë³´ì™„
- ê¸°ì–µ ê³„ë³´ ì¶”ì 
- ìê¸° vs íƒ€ì¸ íšŒìƒ ë¶„ë¦¬

"""

from aura_system.meta_store import (
    search_atoms_by_tags,
    get_fade_candidates,
    get_reflex_memories,
    get_memory_lineage
)
from aura_system.memory_structurer import load_memory_db
from openai import OpenAI
import os

# í†µí•© API í‚¤ ê²€ìƒ‰ ì‚¬ìš©
api_key = _get_valid_openai_key() if '_get_valid_openai_key' in globals() else os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

# âœ… ìš°ì„ ìˆœìœ„ íšŒìƒ íŒë‹¨ ë° ì‹¤í–‰
def hybrid_recall(user_input: str, tags: list, atom_id: str = None, context: dict = None, emotion: dict = None, belief: dict = None, wisdom: dict = None, eora: dict = None, system: dict = None) -> dict:
    result = {
        "reflex": [],
        "direct": [],
        "fading": [],
        "lineage": [],
        "fallback": "",
        "context": context,
        "emotion": emotion,
        "belief": belief,
        "wisdom": wisdom,
        "eora": eora,
        "system": system
    }

    # 1. ì¦‰ì‹œ ë°˜ì‘ ê¸°ì–µ ìš°ì„  íƒìƒ‰
    for word in tags:
        reflex_hits = get_reflex_memories(word)
        if reflex_hits:
            result["reflex"].extend(reflex_hits)

    # 2. íƒœê·¸ ê¸°ë°˜ ì •ê·œ íšŒìƒ
    result["direct"] = search_atoms_by_tags(tags, limit=5)

    # 3. ë§ê° ê²½ê³„ì„ ì— ìˆëŠ” ì¤‘ìš”ì¹˜ ì•Šì€ ê¸°ì–µ í™•ì¸
    result["fading"] = get_fade_candidates(threshold=0.85)

    # 4. ê³„ë³´ ì¶”ì  (ì„ íƒì )
    if atom_id:
        result["lineage"] = get_memory_lineage(atom_id)

    # 5. íšŒìƒ ì‹¤íŒ¨ ì‹œ ìœ ì‚¬ ë³´ì™„ ì œì•ˆ
    if not result["direct"] and not result["reflex"]:
        messages = [
            {"role": "system", "content": "ê³¼ê±°ì˜ ëŒ€í™”ë¥¼ ê¸°ì–µí•  ìˆ˜ ì—†ë‹¤ë©´ ë¹„ìŠ·í•œ ì´ì•¼ê¸°ë¥¼ ìƒìƒí•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•´ì¤˜."},
            {"role": "user", "content": user_input}
        ]
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
        if context:
            messages.append({"role": "system", "content": f"[ì»¨í…ìŠ¤íŠ¸]\n{context}"})
        
        # ê°ì • ì •ë³´ ì¶”ê°€
        if emotion:
            messages.append({"role": "system", "content": f"[ê°ì •]\n{emotion}"})
        
        # ì‹ ë… ì •ë³´ ì¶”ê°€
        if belief:
            messages.append({"role": "system", "content": f"[ì‹ ë…]\n{belief}"})
        
        # ì§€í˜œ ì •ë³´ ì¶”ê°€
        if wisdom:
            messages.append({"role": "system", "content": f"[ì§€í˜œ]\n{wisdom}"})
        
        # ì´ì˜¤ë¼ ì •ë³´ ì¶”ê°€
        if eora:
            messages.append({"role": "system", "content": f"[ì´ì˜¤ë¼]\n{eora}"})
        
        # ì‹œìŠ¤í…œ ì •ë³´ ì¶”ê°€
        if system:
            messages.append({"role": "system", "content": f"[ì‹œìŠ¤í…œ]\n{system}"})
        
        try:
            completion = client.chat.completions.create(
                model="gpt-4",
                messages=messages,
                max_tokens=300
            )
            result["fallback"] = completion.choices[0].message.content
        except Exception as e:
            result["fallback"] = f"[GPT fallback ì˜¤ë¥˜]: {str(e)}"

    return result